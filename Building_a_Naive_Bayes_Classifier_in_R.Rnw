\documentclass[12pt]{article}         % the type of document and font size (default 10pt)
\usepackage[margin=1.0in]{geometry}   % sets all margins to 1in, can be changed
\usepackage{moreverb}                 % for verbatimtabinput -- LaTeX environment
\usepackage{url}                      % for \url{} command
\usepackage{amssymb}                  % for many mathematical symbols
\usepackage[pdftex]{lscape}           % for landscaped tables
\usepackage{longtable}                % for tables that break over multiple pages
\title{Building a Naive Bayes Classifier in R}  % to specify title
\author{BUKIRWA SONIA ANITA MUSOKE}          % to specify author(s)
\begin{document}                      % document begins here
\SweaveOpts{concordance=TRUE}

% If .nw file contains graphs: To specify that EPS/PDF graph files are to be 
% saved to 'graphics' sub-folder
%     NOTE: 'graphics' sub-folder must exist prior to Sweave step
%\SweaveOpts{prefix.string=graphics/plot}

% If .nw file contains graphs: to modify (shrink/enlarge} size of graphics 
% file inserted
%         NOTE: can be specified/modified before any graph chunk
\setkeys{Gin}{width=1.0\textwidth}

\maketitle              % makes the title
%\tableofcontents        % inserts TOC (section, sub-section, etc numbers and titles)
%\listoftables           % inserts LOT (numbers and captions)
%\listoffigures          % inserts LOF (numbers and captions)
%                        %     NOTE: graph chunk must be wrapped with \begin{figure}, 
%                        %  \end{figure}, and \caption{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Where everything else goes

\section{Getting the Data}
The dataset to be used is the \textbf{Iris}. The dataset is divided into \textbf{Training} and \textbf{Testing}. The training sample dataset is shown.
<<echo = FALSE>>=
training <- read.csv('https://raw.githubusercontent.com/selva86/datasets/master/iris_train.csv')
head(training)
@
The testing dataset is as shown:
<<echo = FALSE>>=
test <- read.csv('https://raw.githubusercontent.com/selva86/datasets/master/iris_test.csv')
head(test)
@

\section*{Building the model}
The model is created using klar, and then predict is used to create a prediction from the test dataset.
<<echo = FALSE>>=
library(klaR)
nb_mod <- NaiveBayes(Species ~ ., data=training)
pred <- predict(nb_mod, test)
@

\section*{Evaluation of the Model}
The confusion matrix is used to evaluate the model. A table is created that includes the class from the dataset and the class from the model.

<<echo = FALSE>>=
tab <- table(pred$class, test$Species)
head(tab)
@

The models are evaluated using using the confusion matrix.

<<echo = FALSE>>=
library(caret)
caret::confusionMatrix(tab)  
@

\begin{figure}
<<echo = FALSE, fig = TRUE>>=
opar = par(mfrow=c(2, 2), mar=c(4,0,0,0))
plot(nb_mod, main="")  
par(opar)
@
\caption{Density Plot}
\end{figure}

\begin{figure}
<<echo = FALSE, fig = TRUE>>=
library(ggplot2)
test$pred <- pred$class
ggplot(test, aes(Species, pred, color = Species)) +
  geom_jitter(width = 0.2, height = 0.1, size=2) +
  labs(title="Confusion Matrix", 
       subtitle="Predicted vs. Observed from Iris dataset", 
       y="Predicted", 
       x="Truth")
@
\caption{Plot of Confusion Matrix}
\end{figure}

\vfill
\section*{Summary}
A confusion matrix is also used to determine the number of true and false positives generated by our prediction. For the results to be statistically significant, the P-Value must be less than 0.05 and from the results shown, the P-Value is 8.467e-16, which means that the results are statistically significant. 

Out of 45 being selected from the test data set(setosa, versicolor and viriginica), there were 4 false positives. 4 versicolor species were wrongly predicted to be virginica. 15 true positives of setosa, 11 true positives of versicolor and 15 true positives of virginica were correctly predicted. Therefore, the accuracy of the model is 0.9111.
 
Due to the high accuracy of the model, it can be seen that the model's abiltiy to make predictations in R is very relaible and can be used in the future for more predictions such as these.

\section*{PART 2: PREDICT HUMAN ACTIVITY RECOGNITION (HAR) }
The dataset to be used is the \textbf{HAR dataset}. The dataset is divided into \textbf{Training} and \textbf{Testing}. The  \textbf{training} sample dataset is shown.
<<echo = FALSE>>=
library(caret)
training <- read.csv('https://raw.githubusercontent.com/selva86/datasets/master/har_train.csv')
head(training)
@

<<echo = FALSE, results = tex>>=
library(xtable)
xtable(head(training))
@


The  \textbf{testing} dataset is as shown:
<<echo = FALSE>>=
test <- read.csv('https://raw.githubusercontent.com/selva86/datasets/master/har_validate.csv')
head(test)
@

<<echo = FALSE, results = tex>>=
library(xtable)
xtable(head(test))
@

\section*{Building the model}
The model is created using klar, and then predict is used to create a prediction from the test dataset.
<<echo = FALSE>>=
library(klaR)
nb_mod <- NaiveBayes(classe ~ ., data=training, fL=1, usekernel = T)
pred <- suppressWarnings(predict(nb_mod, test))
@

\subsection*{Confusion Matrix}
The confusion matrix is used to evaluate the model. A table is created that includes the class from the dataset and the class from the model.

<<echo = FALSE>>=
tab <- table(pred$class, test$classe)
@
The models are evaluated using using the confusion matrix.

<<echo = FALSE>>=
caret::confusionMatrix(tab)
@

\begin{figure}
<<echo = FALSE, fig = TRUE>>=
library(ggplot2)
test$pred <- pred$class
ggplot(test, aes(classe, pred, color = classe)) +
  geom_jitter(width = 0.3, height = 0.3, size=1) +
  labs(title="Confusion Matrix", 
       subtitle="Predicted vs. Observed from HAR dataset", 
       y="Predicted", 
       x="Truth")
@
\caption{Plot of Confusion Matrix}
\end{figure}

\vfill
\section*{PART 3: PREDICT THE SQUAREROOT OF A NUMBER}
The dataset being used is the \textbf{SquareRoot dataset}.
<<echo = FALSE>>=
myData <- read.csv("C:/Users/sonia/OneDrive/Desktop/APT3020/SquareRoot.csv",sep=",",header = TRUE)
head(myData)
@

\section*{Dividing the dataset into 80/20}
<<echo = FALSE>>=
set.seed(100)
trainingrow <- sample(1:nrow(myData),0.8*nrow(myData))

@


\subsection*{Selecting the Training Data}
<<echo = FALSE>>=
trainData <- myData[trainingrow,]
head(trainData)
@

\subsection*{Selecting the Testing Data}
<<echo = FALSE>>=
testData<-myData[-trainingrow,]
head(testData)
@

<<echo = FALSE>>=
trainData$SquareRoot <- as.factor(trainData$SquareRoot)
trainData$Number <- as.factor(trainData$Number)

testData$SquareRoot <-as.factor(testData$SquareRoot)
testData$Number <-as.factor(testData$Number)
@
 
\section*{Building the Model}
The model is created using klar and caret liraries. We use predict to create a prediction from the test dataset.
<<echo = FALSE>>=
library(caret)
library(klaR)
nb_mod <- NaiveBayes(SquareRoot ~ Number, data=trainData)
pred <- suppressWarnings(predict(nb_mod, testData))
@

\section*{Evaluation of the model}

<<echo = FALSE>>=
tabs <- union(pred$class, testData$SquareRoot)
tab<-table(factor(pred$class,tabs),factor(testData$SquareRoot,tabs))
@



\subsection*{Confusion Matrix}
The confusion matrix is used to evaluate the model. A table is created that includes the class from the dataset and the class from the model.
The models are evaluated using using the confusion matrix.


<<echo = FALSE>>=
library(caret)
caret::confusionMatrix(tab)
@

\begin{figure}
<<echo = FALSE, fig = TRUE>>=
library(ggplot2)
test$pred <- pred$class
ggplot(testData, aes(factor(testData$SquareRoot), pred, color = factor(testData$SquareRoot))) +
  geom_jitter(width = 0.2, height = 0.1, size=2) +
  labs(title="Confusion Matrix", 
       subtitle="Predicted vs. Observed from Squareroot dataset", 
       y="Predicted", 
       x="Truth")
@
\caption{Plot of Confusion Matrix}
\end{figure}

\section*{Summary}
As seen in the overall statistics from the confusion matrix, the accuracy of the model is 0. This means that the model cannot predict the squareroot of any given number correctly. the P-Value of the model is 1. For the P-value to be statistically significant, the P-value has to be less than 0.05 and for it to be statistically highly significant, it has to be less than 0.001. In conclusion, I would not recommend the use of this model.

In conclusion, I do not recommend using Naive Bayes to build the model classifier because the accuracy that I got for the model was zero, which means tha the model that was built did not correctly predict any number.


\end{document}